{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5da442f9",
   "metadata": {},
   "source": [
    "A python implementation of autodif to show how it works. Based on [this lecture by Alan Edelman.](https://www.youtube.com/watch?v=rZS2LGiurKY&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k&index=36)\n",
    "Autodifferentiation can be represented simply by replacing simple numbers with Tuple objects which track both:\n",
    "* the number $y$\n",
    "* its derivative with respect to $x$, $\\frac {dy} {dx}$\n",
    "\n",
    "Then the derivative propogates *forward* through the calculation just as $y$ does. This representation also shows the usefulness of graph representations of the calculation in machine learning.\n",
    "\n",
    "\n",
    "The below class represents the number with the appended derivative. The first element of the tuple is $y$ and the second is $\\frac {dy} {dx}$. `__mul__`, `__add__`, and `__truediv__` override python `*`, `+`, and `/`. The reason we can create the resulting derivative is because it will be a function of the two operands and their derivatives for all three operations.\n",
    "\n",
    "\n",
    "Notice that we generate a new TupleNum with each calculation, rather than for example updating the values of the existing object. This is equivalent to creating a new node on the graph representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2a5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TupleNum():\n",
    "    \n",
    "    def __init__(self, num):\n",
    "        self.num = num\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        x = self.num[0]*other.num[0]\n",
    "        x_ = self.num[1]*other.num[0] + self.num[0]*other.num[1]\n",
    "        return TupleNum((x, x_))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        x = self.num[0] + other.num[0]\n",
    "        x_ = self.num[1] + other.num[1]\n",
    "        return TupleNum((x, x_))\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        x = self.num[0]/other.num[0]\n",
    "        x_ = (other.num[0]*self.num[1] - self.num[0]*other.num[1])/(other.num[0])**2\n",
    "        return TupleNum((x, x_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c230b",
   "metadata": {},
   "source": [
    "Here's where we see the usefulness of autodifferentiation. Below I implement the [Babylonian square root algorithm](https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method). How it works isn't important, except that it only utilizes the three operations we created: `*`, `+`, and `/`. Notice how the function looks basically the same as it would if we only wanted $y$, but we also get $\\frac {dy} {dx}$, and we get this without ever using the power rule! As we numerically approximate $\\sqrt x$ we also numerically approximate $\\frac {d} {dx} \\sqrt x$ \n",
    "(with 10 iterations it gets the $y$ and $\\frac {dy} {dx}$ to the precision of python floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a213b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def babylonian_root(x, iters=10):\n",
    "    S = x\n",
    "    half = TupleNum((.5, 0))\n",
    "    y = half * x\n",
    "    for i in range(iters):\n",
    "        y = half*(y + S / y)\n",
    "        # print(y.num[0])\n",
    "        # print(y.num[1])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606fa56",
   "metadata": {},
   "source": [
    "`x` is the starting node of the calculation. It's derivative is set to 1, because the derivative of interest, $\\frac {dx} {dx} = 1$. For constants, such as the half used in the Babylonian root algorithm, it's derivative is set to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61c6951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Babylonian approximation: \n",
      "y: 3.162277660168379\n",
      "dy/dx: 0.15811388300841897\n",
      "True values: \n",
      "y: 3.1622776601683795\n",
      "dy/dx: 0.15811388300841897\n"
     ]
    }
   ],
   "source": [
    "x = TupleNum((10, 1))\n",
    "y = babylonian_root(x)\n",
    "\n",
    "print(\"Babylonian approximation: \")\n",
    "print(\"y: \" + str(y.num[0]))\n",
    "print(\"dy/dx: \" + str(y.num[1]))\n",
    "\n",
    "print(\"True values: \")\n",
    "print(\"y: \" + str(10**.5))\n",
    "print(\"dy/dx: \" + str(.5*10**-.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fd582",
   "metadata": {},
   "source": [
    "As a post-note, I should add that, while this is great, forward propagation of derivatives wouldn't be feasible for deep learning. For deep learning we need the derivatives of many, many elements. We couldn't use tuples of just two elements, but each tuple would have to have $y$ as well as its derivative with respect to each element. For thousands and thousands of elements, that's not feasible. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
